{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PORFOLIO - RAGHAV AVASTHI [001873130]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OFFLINE HANDWRITTEN MATH EQUATION ANALYSIS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LICENSE FOR TEXT IN THIS DOCUMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CC-BY-SA\n",
    "© Raghav Avasthi 2018, Some Rights Reserved \n",
    "Except where otherwise noted, this work is licensed under Creative Commons Attribution-ShareAlike 3.0.\n",
    "You are free:\n",
    "   * to Share — to copy, distribute and transmit the work\n",
    "   * to Remix — to adapt the work\n",
    "Under the following conditions:\n",
    "   * Attribution. You must attribute the work in the manner specified by the author or licensor (but not in any way that suggests that they endorse you or your use of the work).\n",
    "   * Share Alike. If you alter, transform, or build upon this work, you may distribute the resulting work only under the same, similar or a compatible license.\n",
    "   \n",
    "   * For any reuse or distribution, you must make clear to others the license terms of this work. The best way to do this is with a link to this web page.\n",
    "   * Any of the above conditions can be waived if you get permission from the copyright holder.\n",
    "   * Nothing in this license impairs or restricts the author's moral rights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### LICENSE FOR CODE IN THIS DOCUMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the document by Raghav Avasthi is licensed under the MIT License https://opensource.org/licenses/MIT\n",
    "https://github.com/RaghavAvasthi/Handwritten-math-equation/blob/master/LICENSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REFERENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) The code is inspired from the following blog and youtube video http://learnandshare645.blogspot.in/2016/06/feeding-your-own-data-set-into-cnn.html https://www.youtube.com/watch?time_continue=1&v=2pQOXjpO_u0\n",
    "\n",
    "2) http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "3) https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My idea is to use Convolutional Neural Networks, specifically Fast-Regional Convolutional Neural Networks to make an offline handwritten mathematical equation recognition system which recognizes individual symbols in math equations in such a robust fashion that it can be used as a dependable recognizer of math symbols from an equation. \n",
    "\n",
    "This problem in the field of computer vision presents a unique kind of difficulty which arises due to context, size and position-based meaning of symbols in the equation; high correlation among the symbols; and due to lack of annotated dataset for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My evaluation is qualitative and well as quantitative\n",
    "\n",
    "3 custom datasets were created of different sizes and number of classes for the purpose of this project. Then, CNN of varying network structure and hyper parameters were applied on 2 custom datasets and their results were quantitatively compared. Finally, F-RCNN were used on them using the Tensorflow Abject Detection API and the results were qualitatively compared for 2 datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach consists of three steps. \n",
    "\n",
    "1) Data set creation. \n",
    "\n",
    "2) Symbol level recognition using Convolutional Neural Networks \n",
    "\n",
    "3) Symbol level recognition using F-RCNN using Tensorflow Object Detection API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) DataSet Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three datasets were created for this project. These datasets were compiled from various other datasets available publicly on the internet. The first dataset named created has 80970 images of size 45x45 pixels with single channel. These images encompass 80 classes i.e. 80 kinds of symbols used in mathematical equations. Second dataset created has 28,782 images of size 32x32 pixels with 3 channels in it i.e. RGB, it is derived from 29 classes of math symbols. The third dataset being the smallest has 16565 single channel images of size 45x45 which are derived from 24 categories of math symbols.\n",
    "\n",
    "All these datasets were formed by combining various datasets having other varied parameters such as size, contrast etc. Thus, to combine these datasets for creating new datasets various techniques were used. First all images were applied with morphological filter for erosion effect to dilate the black ink mark of symbols in images and make it more prominent. It also helped in preserving the outline of the data when resizing images. Then images were zero padded to make them square shaped images accordingly. Images were then resized to the need size i.e. 45x45 and 32x32 in this case. Then contrast filters were used to even the contrast on them. Speckle noise was removed from images using median filter. Images were then turned into single channel binary images as this preserve most of the data in our case and reduces the computation needs for this project. Thereby, reducing time need to train the algorithms. \n",
    "\n",
    "Below is the flow chart for dataset creation\n",
    "[collect data] -> [compile data] -> [erosion] -> [zero padding] -> [resize] -> [contrast adjustment] -> [noise removal] -> binarizing images]\n",
    "After the image dataset was created, 2 kinds of labels were created in the form of a .csv file for these datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codes for making the datasets are in MAATLAB and are given below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MATLAB CODE TO RESIZE IMAGES BY A FACTOR"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "clc\n",
    "clear all\n",
    "factor = 0.25;\n",
    "size = 1400;\n",
    "a = dir(fullfile('C:','[ YOUR OWN PATH OF THE FILE CONTAINING IMAGES]'));\n",
    "i=3;\n",
    "for m = 1:size\n",
    "    b= a(i).name;\n",
    "c=fullfile('C:','[ YOUR OWN PATH OF THE FILE CONTAINING IMAGES]',b);\n",
    "\n",
    "d = imresize(imread(c),factor);\n",
    "imwrite(d,fullfile('C:','[ YOUR OWN PATH OF THE DESTINATION FILE WHERE IMAGES SHOULD BE SAVED ]',b),'tif');\n",
    "i= i+1 \n",
    "end    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MATLAB CODE TO RESIZE TO A SPECIFIC SIZE YOU WANT"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clc\n",
    "clear all\n",
    "rows = 45;\n",
    "cols = 45;\n",
    "\n",
    "direc = dir(fullfile('C:','[ YOUR OWN PATH OF THE FILE CONTAINING IMAGES]'));\n",
    "\n",
    "i=3;\n",
    "for m = 1:81000\n",
    "    im_name= direc(i).name;\n",
    "im_path=fullfile('C:','[ YOUR OWN PATH OF THE FILE CONTAINING IMAGES]',im_name);\n",
    "\n",
    "\n",
    "ee = imresize(im_path,[rows cols]);\n",
    "nm = strrep(im_name,'jpg','png');\n",
    "imwrite(ee,fullfile('C:',[ YOUR OWN PATH OF THE DESTINATION FILE WHERE IMAGES SHOULD BE SAVED ],nm),'png');\n",
    "i= i+1 \n",
    "end   \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MATLAB CODE TO CHANGE IMAGES TO BINARY WITH A THRESHOLD TO DIFFERENTIATE BETWEEN WHITE AND BLACK PIXELS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clc\n",
    "clear all\n",
    "threshold = 0.8;\n",
    "\n",
    "direc = dir(fullfile('C:','[ YOUR OWN PATH OF THE FILE CONTAINING IMAGES]'));\n",
    "\n",
    "i=3;\n",
    "for m = 1:81000\n",
    "    im_name= direc(i).name;\n",
    "im_path=fullfile('C:','[ YOUR OWN PATH OF THE FILE CONTAINING IMAGES]',im_name);\n",
    "\n",
    "\n",
    "d = im2bw(imread(im_path),threshold);\n",
    "\n",
    "nm = strrep(im_name,'jpg','png');\n",
    "imwrite(d,fullfile('C:',[ YOUR OWN PATH OF THE DESTINATION FILE WHERE IMAGES SHOULD BE SAVED ],nm),'png');\n",
    "i= i+1 \n",
    "end   \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MATLAB CODE TO CREATE .csv FILES FOR LABELS OF YOUR DATASET"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clc\n",
    "clear all\n",
    "\n",
    "SIZE = 13550;\n",
    "\n",
    "direc = dir(fullfile('C:',[ YOUR OWN PATH OF THE FILE CONTAINING IMAGES]));\n",
    "\n",
    "\n",
    "struc = struct('filename',{},'width',{},'height',{},'class',{},'xmin',{},'ymin',{},'xmax',{},'ymax',{});\n",
    "\n",
    "i = 3;\n",
    "for m = 1:SIZE\n",
    "    struc(m).xmax = 45;\n",
    "    struc(m).ymax = 45;\n",
    "    struc(m).xmin = 0;\n",
    "    struc(m).ymin = 0;\n",
    "    struc(m).width = 45;\n",
    "    struc(m).height = 45;\n",
    "    struc(m).filename = direc(m).name;\n",
    "    nm = direc(m).name;\n",
    "    k = strfind(nm,' ');\n",
    "    struc(m).class = nm(1:k);\n",
    "    \n",
    "end\n",
    "\n",
    "e = struct2table(struc);\n",
    "writetable(e,'test_labels_small.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MATALB CODE TO REMOVE SPECKLE NOISE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clc\n",
    "clear all\n",
    "size = 81000;\n",
    "\n",
    "direc = dir(fullfile('C:','[ YOUR OWN PATH OF THE FILE CONTAINING IMAGES]'));\n",
    "\n",
    "i=3;\n",
    "for m = 1:size\n",
    "    im_name= direc(i).name;\n",
    "im_path=fullfile('C:','[ YOUR OWN PATH OF THE FILE CONTAINING IMAGES]',im_name);\n",
    "\n",
    "\n",
    "d = medfilt2(imread(im_path));\n",
    "\n",
    "nm = strrep(im_name,'jpg','png');\n",
    "imwrite(d,fullfile('C:',[ YOUR OWN PATH OF THE DESTINATION FILE WHERE IMAGES SHOULD BE SAVED ],nm),'png');\n",
    "i= i+1 \n",
    "end   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MATLAB CODE TO ERODE IMAGE SO THAT THE BLACK INK ON WHITE IMAGES BECOME MORE PROMINENT"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clc\n",
    "clear all\n",
    "size = 81000;\n",
    "\n",
    "direc = dir(fullfile('C:','[ YOUR OWN PATH OF THE FILE CONTAINING IMAGES]'));\n",
    "\n",
    "i=3;\n",
    "for m = 1:size\n",
    "    im_name= direc(i).name;\n",
    "im_path=fullfile('C:','[ YOUR OWN PATH OF THE FILE CONTAINING IMAGES]',im_name);\n",
    "\n",
    "se = offsetstrel('ball',5,5);\n",
    "d = imerode(imread(im_path),se);\n",
    "\n",
    "nm = strrep(im_name,'jpg','png');\n",
    "imwrite(d,fullfile('C:',[ YOUR OWN PATH OF THE DESTINATION FILE WHERE IMAGES SHOULD BE SAVED ],nm),'png');\n",
    "i= i+1 \n",
    "end   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Symbol level recognition using Convolutional Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network  -  In Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple ConvNet is a sequence of layers, and every layer of a ConvNet transforms one volume of activations to another through a differentiable function. We use three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer (exactly as seen in regular Neural Networks). We will stack these layers to form a full ConvNet architecture.\n",
    "\n",
    "Example Architecture: Overview. We will go into more details below, but a simple ConvNet for CIFAR-10 classification could have the architecture [INPUT - CONV - RELU - POOL - FC]. In more detail:\n",
    "\n",
    "INPUT [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.\n",
    "\n",
    "CONV layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters.\n",
    "\n",
    "RELU layer will apply an elementwise activation function, such as the \n",
    "max(0,x)\n",
    "max(0,x)\n",
    "thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]).\n",
    "\n",
    "POOL layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12].\n",
    "\n",
    "FC (i.e. fully-connected) layer will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10. As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.\n",
    "\n",
    "In this way, ConvNets transform the original image layer by layer from the original pixel values to the final class scores. Note that some layers contain parameters and other don’t. In particular, the CONV/FC layers perform transformations that are a function of not only the activations in the input volume, but also of the parameters (the weights and biases of the neurons). On the other hand, the RELU/POOL layers will implement a fixed function. The parameters in the CONV/FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image.\n",
    "\n",
    "\n",
    "In summary:\n",
    "\n",
    "A ConvNet architecture is in the simplest case a list of Layers that transform the image volume into an output volume (e.g. holding the class scores)\n",
    "\n",
    "There are a few distinct types of Layers (e.g. CONV/FC/RELU/POOL are by far the most popular)\n",
    "\n",
    "Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function\n",
    "\n",
    "Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don’t)\n",
    "\n",
    "Each Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesn’t)\n",
    "\n",
    "\n",
    "To study more on CNNs I would suggest the readers to go through the link below\n",
    "http://cs231n.github.io/convolutional-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network for \"train_RGB_small\" dataset - for 3 channel image dataset in general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code given below is used to train a CNN on \"train_RGB_small\" dataset but you can use it to train on your own dataset just by\n",
    "\n",
    "1) saving all your images in a single folder \n",
    "\n",
    "2) Changing the label list in this code according to your own dataset. The indeces are the start and end number of a class in the list \"listing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing all the necessary libraries\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop,adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "\n",
    "\n",
    "from numpy import *\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# Declaring initial variables for training\n",
    "\n",
    "path2 = '[ PATH WHERE YOU HAVE DOWNLOADED THE TRAIN_RGB_SMALL DATASET ]/train_RGB_small' # put the path where you have downloaded the train_RGB_small dataset\n",
    "\n",
    "img_rows, img_cols = 32, 32 # this dedfines the size of images you are using\n",
    "\n",
    "img_channels = 3  # number of channels in the image i.e. put 3 if image is RGB or else put 1 if image is single channel\n",
    "\n",
    "batch_size = 32  # define the batch size for training\n",
    "\n",
    "nb_classes = 31 # define the number of classes\n",
    "\n",
    "nb_epoch = 20 # the number of epochs you would like to train\n",
    "\n",
    "nb_filters = 32 # number of filters used in the convolution layer \n",
    "\n",
    "nb_pool = 2  # size of the pooling function i.e. 2x2 here, in the pooling layer\n",
    "\n",
    "nb_conv = 3 # size of the convolution filters\n",
    "\n",
    "## NOTE: this is my default setting for hyperparameters which I have kept common for all my layers. If you wish to use different values for different layers, please edit them where network is declared\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "#listing and sorting images by name\n",
    "\n",
    "listing = os.listdir(path2)\n",
    "listing = sorted(listing) # sorted is a function imported from numpy library, it sorts the \"listing\" list in alpha-numeric order\n",
    "num_samples=size(listing)\n",
    "\n",
    "##################################################################################################################################\n",
    "\n",
    "#Storing all images in a single matrix and reshaping it for training\n",
    "\n",
    "ii = 0\n",
    "immatrix = np.zeros((num_samples,img_channels,img_rows,img_cols)) # create a 4-dimension 0 matrix of size along 1st dimension equal to number of images\n",
    "for file in listing:\n",
    "    im2 = cv2.imread(path2+ '/' + file) # filling images from our dataset to the \"immatrix\"\n",
    "    im2 = im2.reshape(img_channels,img_rows,img_cols) # reshaping it to flatten it so that it can be easiy fed to a CNN\n",
    "    immatrix[ii,0:img_channels,0:img_rows,0:img_cols] = im2 # resulant is a list of arrays long enough where each array represent an image\n",
    "    ii = ii + 1\n",
    "    \n",
    "# This part of code helps to store all the images in a 4-dimension matrix, along with its 1st dimension\n",
    "\n",
    "##################################################################################################################################\n",
    "\n",
    "#Preparing label matrix for our image dataset\n",
    "\n",
    "label=np.ones((num_samples,),dtype = int)\n",
    "label[0:1332]=0\n",
    "label[1332:3036]=3\n",
    "label[3036:4181]=4\n",
    "label[4181:4908]=5\n",
    "label[4908:5557]=6\n",
    "label[5557:6229]=7\n",
    "label[6229:6823]=8\n",
    "label[6823:7472]=9\n",
    "label[7472:8570]=10 #B\n",
    "label[8570:10382]=11 #a\n",
    "label[10382:11487]=12 #alpha\n",
    "label[11487:11667]=13 #beta\n",
    "label[11667:22029]=14 #bracket\n",
    "label[22029:22270]=15 #cos\n",
    "label[22270:22745]=16 #d\n",
    "label[22745:23090]=17#e\n",
    "label[23090:23369]=18#f\n",
    "label[23369:23575]=19#g\n",
    "label[23575:23618]=20#gamma\n",
    "label[23618:23712]=21#h\n",
    "label[23712:24606]=22#i\n",
    "label[24606:24733]=23#infinity\n",
    "label[24733:25215]=24#intigrate\n",
    "label[25215:25485]=25#lim\n",
    "label[25485:26030]=26#log\n",
    "label[26030:26274]=27#m\n",
    "label[26274:27958]=28#n\n",
    "label[27958:28442]=29#p\n",
    "label[28442:]=30#pi\n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "#Matching image data with label data and randomizing it\n",
    "\n",
    "data,Label = shuffle(immatrix,label, random_state=2) # this is don\n",
    "data,Label = shuffle(immatrix,label, random_state=2) # this is done to randomize our data as when we split it into train and test data, both train an test data shold get a mix of all the classes.\n",
    "train_data = [data,Label]\n",
    "(X, y) = (train_data[0],train_data[1])\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "#Spliting the data into Train and Test datasets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "# Here I have split the data into 80% train dataset and 20% test dataset along with a little randomization. \n",
    "#To alter the split percentages, alter the \"test_size\" variable. It ranges from 0.9 to 0.1, recommended: 0.6 - 0.8\n",
    "\n",
    "###################################################################################################################################\n",
    "\n",
    "# Typecasting and normalizing the dataset for reducing the computational needs for training\n",
    "\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train)\n",
    "Y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# Doing this helps normalize the dataset\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "# Define your Convolutional Neural Network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(nb_filters, (nb_conv, nb_conv), padding='same',input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu')) # activation functions bring non-linearity to the model\n",
    "\n",
    "model.add(Convolution2D(nb_filters, (nb_conv, nb_conv), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "model.add(Dropout(0.25)) # dropout layer prevents overfitting and improves speed and accuracy\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax')) # soft max helps to calculate the probabilities\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta',metrics =[\"accuracy\"])\n",
    "\n",
    "#############################################################################################################################\n",
    "\n",
    "#Train your Convolutional Neural Network\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,verbose=1, validation_data=(X_test, Y_test))\n",
    "\n",
    "###############################################################################################################################\n",
    "\n",
    "#Print scores of your network\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network for \"train_single\" dataset  -  For 1 channel image dataset in general\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code given below is used to train a CNN on \"train_single\" dataset but you can use it to train on your own dataset just by\n",
    "\n",
    "1) saving all your images in a single folder \n",
    "\n",
    "2) Changing the label list in this code according to your own dataset. The indeces are the start and end number of a class in the list \"listing\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing all the necessary libraries\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop,adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "\n",
    "\n",
    "from numpy import *\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# Declaring initial variables for training\n",
    "\n",
    "img_rows, img_cols = 45, 45 # this dedfines the size of images you are using\n",
    "\n",
    "img_channels = 1 # number of channels in the image i.e. put 3 if image is RGB or else put 1 if image is single channel\n",
    "\n",
    "batch_size = 45 # define the batch size for training\n",
    "\n",
    "nb_classes = 80 # define the number of classes\n",
    "\n",
    "nb_epoch = 100 # the number of epochs you would like to train\n",
    "\n",
    "nb_filters = 8 # number of filters used in the convolution layer \n",
    "\n",
    "nb_pool = 2 # size of the pooling function i.e. 2x2 here, in the pooling layer\n",
    "\n",
    "nb_conv = 2 # size of the convolution filters\n",
    "\n",
    "path2 = '[ PATH WHERE YOU HAVE DOWNLOADED THE TRAIN_SINGLE DATASET ]/train_single' \n",
    "\n",
    "## NOTE: this is my default setting for hyperparameters which I have kept common for all my layers. If you wish to use different values for different layers, please edit them where network is declared\n",
    "###########################################################################################################################\n",
    "\n",
    "#listing and sorting images by name\n",
    "\n",
    "listing = os.listdir(path2)\n",
    "listing = sorted(listing) # sorted is a function imported from numpy library, it sorts the \"listing\" list in alpha-numeric order\n",
    "num_samples=size(listing)\n",
    "\n",
    "##################################################################################################################################\n",
    "\n",
    "#Storing all images in a single matrix and reshaping it for training\n",
    "\n",
    "immatrix = array([array(Image.open(path2+ '/' + im2)).flatten()for im2 in listing],'f')\n",
    "#reshaping it to flatten it so that it can be easiy fed to a CNN\n",
    "# resulant is a list of arrays long enough where each array represent an image\n",
    "\n",
    "##################################################################################################################################\n",
    "\n",
    "#Preparing label matrix for our image dataset\n",
    "\n",
    "label=np.ones((num_samples,),dtype = int)\n",
    "label[0:1298]=10#!\n",
    "label[1298:2818]=11#(\n",
    "label[2818:4141]=12#)\n",
    "label[4141:5383]=13#+\n",
    "label[5383:6076]=14#,\n",
    "label[6076:8002]=15#-\n",
    "label[8002:9037]=0#0\n",
    "label[9037:10356]=1\n",
    "label[10356:11763]=2 \n",
    "label[11763:12808]=3 \n",
    "label[12808:13772]=4 \n",
    "label[13772:14772]=5 \n",
    "label[14772:15979]=6 \n",
    "label[15979:16952]=7\n",
    "label[16952:17917]=8\n",
    "label[17917:19151]=9\n",
    "label[19151:20222]=17#=\n",
    "label[20222:21000]=18#[\n",
    "label[21000:21780]=19#]\n",
    "label[21780:23302]=20#a\n",
    "label[23302:24011]=21#alpha\n",
    "label[24011:24795]=22#b\n",
    "label[24795:25326]=23#beta\n",
    "label[25326:26128]=24#c\n",
    "label[26128:26912]=25#cos\n",
    "label[26912:27723]=26#d\n",
    "label[27723:27860]=27#delta\n",
    "label[27860:28728]=28#div\n",
    "label[28728:29828]=29#e\n",
    "label[29828:29849]=30#exists \n",
    "label[29849:30949]=31#f \n",
    "label[30949:31148]=32#for_slash \n",
    "label[31148:31193]=33#forall \n",
    "label[31193:32885]=34#g \n",
    "label[32885:33294]=35#gamma\n",
    "label[33294:33987]=36#geq\n",
    "label[33987:34245]=37#gt\n",
    "label[34245:35709]=38#h\n",
    "label[35709:37033]=39#i \n",
    "label[37033:37080]=40#in \n",
    "label[37080:38863]=41#infty \n",
    "label[38863:39844]=42#int \n",
    "label[39844:41380]=43#j \n",
    "label[41380:42557]=44#k\n",
    "label[42557:43574]=45#l\n",
    "label[43574:43683]=46#lambda\n",
    "label[43683:44656]=47#leq\n",
    "label[44656:46331]=48#lim\n",
    "label[46331:48332]=49#log \n",
    "label[48332:48809]=50#lt \n",
    "label[48809:50077]=51#m \n",
    "label[50077:50254]=52#mu \n",
    "label[50254:51641]=53#n\n",
    "label[51641:52199]=54#neq\n",
    "label[52199:52648]=55#o\n",
    "label[52648:54105]=56#p\n",
    "label[54105:54460]=57#phi \n",
    "label[54460:55588]=58#pi \n",
    "label[55588:56390]=59#pm \n",
    "label[56390:57620]=60#q \n",
    "label[57620:59105]=61#r \n",
    "label[59105:60808]=62#rgt_arw\n",
    "label[60808:62221]=63#s\n",
    "label[62221:62422]=64#sigma\n",
    "label[62422:63571]=65#sin\n",
    "label[63571:65570]=66#sqrt \n",
    "label[65570:66901]=67#sum \n",
    "label[66901:68463]=68#t \n",
    "label[68463:69780]=69#tan \n",
    "label[69780:71097]=70#theta \n",
    "label[71097:72694]=71#times\n",
    "label[72694:73963]=72#u\n",
    "label[73963:75521]=73#v\n",
    "label[75521:76077]=74#w\n",
    "label[76077:77751]=75#x\n",
    "label[77751:79250]=76#y\n",
    "label[79250:80217]=77#z\n",
    "label[80217:80593]=78#{\n",
    "label[80593:]=16#}\n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "data,Label = shuffle(immatrix,label, random_state=2) # this is done to randomize our data as when we split it into train and test data, both train an test data shold get a mix of all the classes.\n",
    "train_data = [data,Label]\n",
    "(X, y) = (train_data[0],train_data[1])\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "#Spliting the data into Train and Test datasets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "# Here I have split the data into 80% train dataset and 20% test dataset along with a little randomization. \n",
    "#To alter the split percentages, alter the \"test_size\" variable. It ranges from 0.9 to 0.1, recommended: 0.6 - 0.8\n",
    "\n",
    "###################################################################################################################################\n",
    "\n",
    "# Typecasting and normalizing the dataset for reducing the computational needs for training\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_cols, img_rows, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_cols, img_rows, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "# Doing this helps normalize the dataset\n",
    "##############################################################################################################################\n",
    "\n",
    "# Define your Convolutional Neural Network\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv,border_mode='valid',input_shape=(img_cols, img_rows, 1)))\n",
    "convout1 = Activation('relu') # activation functions bring non-linearity to the model\n",
    "model.add(convout1)\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "convout2 = Activation('relu')\n",
    "model.add(convout2)\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "model.add(Dropout(0.5))   # dropout layer prevents overfitting and improves speed and accuracy\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))  # soft max helps to calculate the probabilities\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "\n",
    "#############################################################################################################################\n",
    "\n",
    "#Train your Convolutional Neural Network\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,verbose=1, validation_data=(X_test, Y_test))\n",
    "\n",
    "###############################################################################################################################\n",
    "\n",
    "#Print scores of your network\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Symbol level recognition using Fast - Regional Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run F-RCNN on my own dataset I used Tensorflow's Object Detection API. I tweaked it according to my preferences and used it.\n",
    "\n",
    "Using Tensorflow's Object Detection API takes a lot of installation and a lot of steps due to which I feel that expalining the whole API or even just the relevant part of it is a task probably beyond the scope of this portfolio. That is why I would suggest the readers to go through the links given below. \n",
    "\n",
    "To simplify its uasge I have uploaded the prepared datasets and their csv files on my github page i.e. https://github.com/RaghavAvasthi/Handwritten-math-equation\n",
    "\n",
    "This would leave you with just \n",
    "\n",
    "1) downloading and insatlling the required libraries\n",
    "\n",
    "2) running and checking your installation\n",
    "\n",
    "3) Running the code \n",
    "\n",
    "These things can be done easily by going through the links below\n",
    "\n",
    "It is very helpful and has a youtube video to support it as well\n",
    "\n",
    "the link of the blog is : https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10\n",
    "\n",
    "the link of the youtube video is : https://www.youtube.com/watch?v=Rgpfk6eYxJA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Results for applying CNN on the 2 datasets are given below. \n",
    "Epoch = 20\n",
    "            \tDataset 1\t   Dataset 2\n",
    "Network 1\tDid not converge\t84.82%\n",
    "Network 2\tDid not converge\t87.46%\n",
    "\n",
    "Increasing the epochs to 350 for both the datasets\n",
    " Epoch = 350\n",
    "                \tDataset 1\tDataset 2\n",
    "Network 1\tDid not converge\t92.82%\n",
    "Network 2\tDid not converge\t94.46%\n",
    "\n",
    "Since graphic results are not possible on jupyter notebook, I would rwquest the readers to go through the research paper uploaded on my GitHub page:  https://github.com/RaghavAvasthi/Handwritten-math-equation\n",
    "\n",
    "As results are shown there graphically\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results I can conclude that Convolutional Neural Networks may be suitable for symbol level detection but cannot be used for expression level detection as the accuracy achieved for symbol level classification is comparable to industry standards and can be used for practical purposes but the accuracy in recognizing a handwritten math equation from non-interactive offline sources such as images still has a lot of potential of improvement. As the accuracy in this case is \n",
    "\n",
    "Acc. = (928/1000)n  x 100\n",
    "\n",
    "This accuracy for recognizing offline handwritten math equation drops below 50% if there are 10 or more symbols in an equation. It is worth noticing that it goes below 80% for 3 or more symbols in an equation.\n",
    "\n",
    "In case of F-RCNN, it can be concluded that F-RCNNs can be used for symbol level and expression level detection of offline handwritten mathematical equations but only if the number of classes used to train are conveniently less. It is shown from the result that F-RCNN failed to recognize symbols and failed to converge with the Tensorflow Object detection API with Dataset 1 as it has 80 classes while they were giving satisfactory results when trained with dataset 3 which has just 24 classes.\n",
    "Therefore, it can be said that the hypothesis failed and was wrong.\n",
    "\n",
    "The reason for such high inaccuracy can be attributed to high correlation among symbols in mathematics which can only be resolved by taking the size, position of a symbol in an equation and the context in which it is written.\n",
    "\n",
    "From my work on this project I can also conclude that applying a few preprocessing techniques such as padding, resizing, erosion and contrast enhancement help in making the data consistent and uniform thereby increasing the accuracy on the CNN algorithm and reducing time in training it.\n",
    "\n",
    "Changing the images to 1 channel, black and white image from a 3-channel RGB images also increases the speed in training and reduces the memory requirements.\n",
    "\n",
    "Using techniques such as Data Augmentation and Dropout layer further avoids overfitting and improve the accuracy of the algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several improvements can be made in the future works of this project. \n",
    "Differentiating between compound and simple symbols and training on them separately can help as that would reduce the overlapping as shown in figure 4. Using tree-based techniques or other deep learning algorithms such as YOLO may also help. Training the data on a better GPU for a longer duration will also improve the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
