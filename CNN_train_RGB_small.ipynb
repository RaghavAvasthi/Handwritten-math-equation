{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network code to run on the dataset \"train_RGB_small\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyPrograms\\Anaconda3\\envs\\tensor35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\MyPrograms\\Anaconda3\\envs\\tensor35\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop,adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "\n",
    "\n",
    "from numpy import *\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring initial variables for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = 'C:/Users/Raghav Avasthi/Documents/Adv. Data Science/train_RGB_small' # put the path where you have downloaded the train_RGB_small dataset\n",
    "\n",
    "img_rows, img_cols = 32, 32 # this dedfines the size of images you are using\n",
    "\n",
    "img_channels = 3  # number of channels in the image i.e. put 3 if image is RGB or else put 1 if image is single channel\n",
    "\n",
    "batch_size = 32  # define the batch size for training\n",
    "\n",
    "nb_classes = 31 # define the number of classes\n",
    "\n",
    "nb_epoch = 20 # the number of epochs you would like to train\n",
    "\n",
    "nb_filters = 32 # number of filters used in the convolution layer \n",
    "\n",
    "nb_pool = 2  # size of the pooling function i.e. 2x2 here, in the pooling layer\n",
    "\n",
    "nb_conv = 3 # size of the convolution filters\n",
    "\n",
    "## NOTE: this is my default setting for hyperparameters which I have kept common for all my layers. If you wish to use different values for different layers, please edit them where network is declared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### listing and sorting images by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing = os.listdir(path2)\n",
    "listing = sorted(listing)\n",
    "num_samples=size(listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing all images in a single matrix and reshaping it for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = 0\n",
    "immatrix = np.zeros((num_samples,img_channels,img_rows,img_cols))\n",
    "for file in listing:\n",
    "    im2 = cv2.imread(path2+ '/' + file)\n",
    "    im2 = im2.reshape(img_channels,img_rows,img_cols)\n",
    "    immatrix[ii,0:img_channels,0:img_rows,0:img_cols] = im2\n",
    "    ii = ii + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing label matrix for our image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=np.ones((num_samples,),dtype = int)\n",
    "label[0:1332]=0\n",
    "label[1332:3036]=3\n",
    "label[3036:4181]=4\n",
    "label[4181:4908]=5\n",
    "label[4908:5557]=6\n",
    "label[5557:6229]=7\n",
    "label[6229:6823]=8\n",
    "label[6823:7472]=9\n",
    "label[7472:8570]=10 #B\n",
    "label[8570:10382]=11 #a\n",
    "label[10382:11487]=12 #alpha\n",
    "label[11487:11667]=13 #beta\n",
    "label[11667:22029]=14 #bracket\n",
    "label[22029:22270]=15 #cos\n",
    "label[22270:22745]=16 #d\n",
    "label[22745:23090]=17#e\n",
    "label[23090:23369]=18#f\n",
    "label[23369:23575]=19#g\n",
    "label[23575:23618]=20#gamma\n",
    "label[23618:23712]=21#h\n",
    "label[23712:24606]=22#i\n",
    "label[24606:24733]=23#infinity\n",
    "label[24733:25215]=24#intigrate\n",
    "label[25215:25485]=25#lim\n",
    "label[25485:26030]=26#log\n",
    "label[26030:26274]=27#m\n",
    "label[26274:27958]=28#n\n",
    "label[27958:28442]=29#p\n",
    "label[28442:]=30#pi\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching image data with label data and randomizing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,Label = shuffle(immatrix,label, random_state=2)\n",
    "train_data = [data,Label]\n",
    "(X, y) = (train_data[0],train_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the data into Train and Test datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "# Here I have split the data into 80% train dataset and 20% test dataset along with a little randomization. \n",
    "#To alter the split percentages, alter the \"test_size\" variable. It ranges from 0.9 to 0.1, recommended: 0.6 - 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typecasting and normalizing the dataset for reducing the computational needs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train)\n",
    "Y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# Doing this helps normalize the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(nb_filters, (nb_conv, nb_conv), padding='same',input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Convolution2D(nb_filters, (nb_conv, nb_conv), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta',metrics =[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23025 samples, validate on 5757 samples\n",
      "Epoch 1/20\n",
      "  128/23025 [..............................] - ETA: 31s - loss: 0.5538 - acc: 0.8828"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyPrograms\\Anaconda3\\envs\\tensor35\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23025/23025 [==============================] - 22s 953us/step - loss: 0.6067 - acc: 0.8406 - val_loss: 0.8134 - val_acc: 0.8098\n",
      "Epoch 2/20\n",
      "23025/23025 [==============================] - 22s 942us/step - loss: 0.5889 - acc: 0.8458 - val_loss: 0.6655 - val_acc: 0.8289\n",
      "Epoch 3/20\n",
      "23025/23025 [==============================] - 21s 933us/step - loss: 0.5836 - acc: 0.8496 - val_loss: 0.6682 - val_acc: 0.8352\n",
      "Epoch 4/20\n",
      "23025/23025 [==============================] - 22s 941us/step - loss: 0.5813 - acc: 0.8480 - val_loss: 0.6549 - val_acc: 0.8372\n",
      "Epoch 5/20\n",
      "23025/23025 [==============================] - 22s 952us/step - loss: 0.5736 - acc: 0.8505 - val_loss: 0.6703 - val_acc: 0.8385\n",
      "Epoch 6/20\n",
      "23025/23025 [==============================] - 22s 939us/step - loss: 0.5738 - acc: 0.8521 - val_loss: 0.6668 - val_acc: 0.8416\n",
      "Epoch 7/20\n",
      "23025/23025 [==============================] - 22s 956us/step - loss: 0.5727 - acc: 0.8506 - val_loss: 0.6609 - val_acc: 0.8405\n",
      "Epoch 8/20\n",
      "23025/23025 [==============================] - 22s 974us/step - loss: 0.5548 - acc: 0.8578 - val_loss: 0.6806 - val_acc: 0.8352\n",
      "Epoch 9/20\n",
      "23025/23025 [==============================] - 24s 1ms/step - loss: 0.5533 - acc: 0.8572 - val_loss: 0.6612 - val_acc: 0.8372\n",
      "Epoch 10/20\n",
      "23025/23025 [==============================] - 22s 972us/step - loss: 0.5535 - acc: 0.8578 - val_loss: 0.6449 - val_acc: 0.8433\n",
      "Epoch 11/20\n",
      "23025/23025 [==============================] - 22s 967us/step - loss: 0.5507 - acc: 0.8568 - val_loss: 0.6731 - val_acc: 0.8402\n",
      "Epoch 12/20\n",
      "23025/23025 [==============================] - 22s 970us/step - loss: 0.5426 - acc: 0.8600 - val_loss: 0.6775 - val_acc: 0.8310\n",
      "Epoch 13/20\n",
      "23025/23025 [==============================] - 23s 985us/step - loss: 0.5486 - acc: 0.8567 - val_loss: 0.6913 - val_acc: 0.8346\n",
      "Epoch 14/20\n",
      "23025/23025 [==============================] - 23s 1ms/step - loss: 0.5409 - acc: 0.8609 - val_loss: 0.7028 - val_acc: 0.8260\n",
      "Epoch 15/20\n",
      "23025/23025 [==============================] - 23s 996us/step - loss: 0.5323 - acc: 0.8611 - val_loss: 0.6503 - val_acc: 0.8379\n",
      "Epoch 16/20\n",
      "23025/23025 [==============================] - 23s 984us/step - loss: 0.5307 - acc: 0.8615 - val_loss: 0.6839 - val_acc: 0.8425\n",
      "Epoch 17/20\n",
      "23025/23025 [==============================] - 23s 1ms/step - loss: 0.5300 - acc: 0.8638 - val_loss: 0.6828 - val_acc: 0.8372\n",
      "Epoch 18/20\n",
      "23025/23025 [==============================] - 23s 1ms/step - loss: 0.5330 - acc: 0.8640 - val_loss: 0.6564 - val_acc: 0.8423\n",
      "Epoch 19/20\n",
      "23025/23025 [==============================] - 20s 872us/step - loss: 0.5242 - acc: 0.8637 - val_loss: 0.6555 - val_acc: 0.8445\n",
      "Epoch 20/20\n",
      "23025/23025 [==============================] - 20s 861us/step - loss: 0.5193 - acc: 0.8669 - val_loss: 0.6474 - val_acc: 0.8442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e5cc806f98>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.3_train_RGB_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print scores of your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8441896821468142\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
